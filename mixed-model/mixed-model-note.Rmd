---
title: "Mixed Model （混合效应模型）"
author: "Ming"
output:
  html_document:
    code_folding: show
    css: style.css
    toc: true
    toc_depth: 2
    df_print: paged
    #theme: united
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,message = F,warning = F,
        fig.width=5,fig.height=5,cache = TRUE,
        #fig.show='hold',
        fig.align='center')
library(ggplot2);library(dplyr);library(purrr);library(gridExtra)
```

# why mixed model

When observations are not independent from each other,
the covariance structure among observations need to be properly modelled.

# terminology

- **Linear mixed model**: a statistical model that is linear in its structure, wherein a focal response variable is considered a function of several variables and/or factors some of which have fixed and others random effects
- **Fixed effect**: a model parameter for which the levels or values are assumed to be fixed (i.e., constant). 
- **Random effect**: a model parameter assumed to represent a random variable. 
- **Pseudoreplication**: occurs when analyzing data as if one had more independent samples, observations, or replicates than is actually the case.
- **Degrees of freedom**: the number of values in the calculation of a metric that are free to vary.
- **ICC**: [intraclass correlation (ICC)](https://stat.ethz.ch/~meier/teaching/anova/random-and-mixed-effects-models.html). A large value means that observations from the same "group" are much more similar than observations from different groups. 

# questiones addressed via MM

- Is there a significant xxx effect (fixed effect) controlling for xxx (random effect)?
- How much variance in response variable can be explained by xx, xx, and/or xxx?

# types of MM
![Figure source: https://bookdown.org/steve_midway/DAR/random-effects.html#when-are-random-effects-appropriate](types_of_MM.png)

`lmer` implementations:

- Fixed effects: `lm`
- Random intercept, fixed slope: `lmer(y ~ x + (1|A), data = df)`, `lmer(y ~  (1|A), data = df)` 
- Fixed intercept, random slope: `lmer(y ~ x + (0+x|A), data = df)`, `lmer(y ~ (0+x|A), data = df)`
- Random intercept, random slope: `lmer(y ~ x + (x|A), data = df)`, `lmer(y ~ (x|A), data = df)`

# typical procedures
- model configuration, specify random effects structure
- model selection
  + [anova via LRT](https://joshuawiley.com/MonashHonoursStatistics/LMM_Comparison.html): use ML (REML=FALSE) when comparing models
  + [AIC](https://www.statology.org/negative-aic//#AIC(lm.fit,model_ints_only)): The lower the value for AIC, the better the fit of the model. The absolute value of the AIC value is not important. It can be positive or negative.
- parameter estimation of the selected model(use REML) and further (biological) interpretation

Note:
*why use ML in model selection?*

[Theory and practice of random effects and REML, Developed by Gabriel Hoffman](https://bioconductor.org/packages/release/bioc/vignettes/variancePartition/inst/doc/theory_practice_random_effects.html)

```
Finally, why use maximum likelihood to estimate the paramters instead of the default REML ()? Maximum likelihood fits all parameters jointly so that it estimates the fixed and random effects together. This is essential if we want to compare fixed and random effects later. 
Conversely, REML estimates the random effects by removing the fixed effects from the response before estimation. This implicitly removes the fixed effects from the denominator when evaluating the variance fraction. REML treats fixed effects as nuisance variables, while variancePartition considers fixed effects to be a core part of the analysis.
```

# testing Significance of Effects 
refer to: https://www.ssc.wisc.edu/sscc/pubs/MM/MM_TestEffects.html

- test of fixed effects (test if the coefficients are zero)
  + LRT (Likelihood Ratio Test.) REML=FALSE, Tests the difference in two nested models using the Chi square distribution.
  + KRmodComp. Only available for linear mixed models (does not support glmer() models.) An F test of nested models with an estimated degrees of freedom. The KRmodcomp() function estimates which F-test distribution is the best distribution from the family of F distributions. This function addresses the degrees of freedom concern. [car::Anova(mymodel, test.statistic='F')](https://rdrr.io/cran/car/man/Anova.html)
  
- test of random parameters (Test variance parameter is equal to 0)
  + LRT (Likelihood Ratio Test)

# Application I: individual-level variation in tree canopy gradients

An example taken from: https://www.hiercourse.com/#workshops

**Question**: Is leaf mass per area (LMA) affected/predicted by distance from the top of the tree (dfromtop)?

- 35 trees of 2 tree species
- each individual tree has multiple measurements

```{R}
library(lme4);
library(tidyverse)
pref <- read.csv("prefdata.csv")
head(pref)
# each id has multiple measurements
pref %>% group_by(species,ID) %>% summarise(n.ind=n())
ggplot(pref,aes(x=dfromtop,y=LMA,group=ID,col=species))+geom_point()+geom_line()+
  facet_wrap(.~ID)+theme_classic()

# Fit a linear regression by species (ignoring individual-level variation)
lm1 <- lm(LMA ~ species + dfromtop + species:dfromtop, data=pref)
# Plot predictions
library(visreg)
visreg(lm1, "dfromtop", by="species", overlay=TRUE)

# fitting mixed models
pref_m1 <- lmer(LMA ~ species + dfromtop + species:dfromtop + (1|ID),REML=FALSE, data=pref)
# Random intercept and slope
pref_m2 <- lmer(LMA ~ species + dfromtop + species:dfromtop + (dfromtop|ID),REML=FALSE,data=pref)
# The AIC and a likelihood-ratio test tell us that we don't need a random slope.
# lower AIC indicates that model fit is better (more efficient)
AIC(pref_m1, pref_m2) #difference >=2 provides evidence for model selection
anova(pref_m1, pref_m2,test = "LRT")

# https://rpubs.com/corey_sparks/420770
library(MuMIn)
model.sel(pref_m1, pref_m2)
# Using Anova from car, we get p-values for the main effects.
car::Anova(pref_m1)
# To visualize these differences, also compare predictions from the mixed-effects model, to those of the fixed-effects model.
visreg(pref_m1, "dfromtop", by="species", overlay=TRUE)

# variance explained via fixed and random effects
# http://www.alexanderdemos.org/Class14.html
library(performance)
r2_nakagawa(pref_m1)
#The marginal is the R2 for the fixed effects
#The conditional R2 includes random + fixed effects

# compare a model with no random effects to a model with a random effect using lme4?
# https://stackoverflow.com/questions/24019807/how-to-compare-a-model-with-no-random-effects-to-a-model-with-a-random-effect-us
library(lme4)
fm1 <- lmer(Reaction~Days+(1|Subject),sleepstudy, REML=FALSE)
fm0 <- lm(Reaction~Days,sleepstudy)
AIC(fm1,fm0)
```

# Application II: plant phenotypic plasticity in response to a chaning climate

An example taken from: https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.15656

**Question**: How does temperature increase affect spring flowering time advancement?

- 20 genotypes, each contain 10 individuals
- All 200 individuals were grown in the same environment for some time, then the 10 individuals were transferred into ten temperature growth cabinets.
- for each genotype: one ind in one temp condition
- for each temp condition: 10 inds each of one genotype

```{R}
library(ggplot2)
library(MuMIn)
library(lme4)


# Read in data and check structure
flowerdata <- read.csv(file = "flowerdata.csv", header = TRUE, skip = 10)
head(flowerdata)
str(flowerdata)

# Convert genotype from integer to factor and check structure again
flowerdata$genotype <- as.factor(flowerdata$genotype)
flowerdata$loc <- as.factor(flowerdata$temperature)
str(flowerdata)

# Mean-centre the x variable (temperature)
# From here on, use the mean-centred temperature (ctemperature)
flowerdata$ctemperature <- scale(flowerdata$temperature)
head(flowerdata)

# Plot the main effects
ggplot(flowerdata, aes(x = ctemperature, y = relativedate, group = genotype)) +
  geom_line(aes(colour = genotype)) + ylab("Relative date of flowering") + 
  xlab("Mean-centred growth temperature") + theme_classic()


#### Basic linear model ####
# Fit a linear model for the fixed effect of growth temperature on relative date of flowering.
# Note that we also add a random effect for 'location' to take account of the 
# repeated measures at each temperature

model1.1 <- lmer(relativedate ~ ctemperature + (1|loc), REML = FALSE, data = flowerdata)

# Check model summary and R squared values
summary(model1.1)
r.squaredGLMM(model1.1)

# Predict values based on the model fit using the predict function
temperature_pred <- data.frame(ctemperature = seq(from = min(flowerdata$ctemperature),
                                                 to = max(flowerdata$ctemperature),
                                                 length.out = 50))
temperature_pred$fit1.1 <- predict(model1.1, newdata = temperature_pred, re.form = NA)

# Plot the raw data and overlay the fit of Model1.1
ggplot(temperature_pred, aes(x = ctemperature, y = fit1.1)) +
  geom_line(data = flowerdata, aes(y = relativedate, colour = genotype)) +
  geom_line(size = 2) +
  ylab("Relative date of flowering") + xlab("Mean-centred growth temperature") +
  theme_classic()


#### Quadratic fixed effects model ####
# Fit a quadratic model for the fixed effect of growth temperature on relative date of flowering
model1.2 <- lmer(relativedate ~ poly(ctemperature, 2, raw = T) + (1|loc), 
                 REML = FALSE, data = flowerdata)

# Check model summary and R squared values
summary(model1.2)
r.squaredGLMM(model1.2)

# Predict values based on the model fit using the predict function
temperature_pred$fit1.2 <- predict(model1.2, newdata = temperature_pred, re.form = NA)

# Plot the overall model fit over the top of the raw data 
ggplot(temperature_pred, aes(x = ctemperature, y = fit1.2)) +
  geom_line(data = flowerdata, aes(y = relativedate, colour = genotype)) +
  geom_line(size = 2) +
  ylab("Relative date of flowering") + xlab("Mean-centred growth temperature") +
  theme_classic()

# Are the two models different? 
# Likelihood ratio test
chi2 <- 2*(summary(model1.2)$logLik - summary(model1.1)$logLik)
1-pchisq(chi2,1)

# AIC comparison
AIC(model1.1, model1.2)


#### Quadratic fixed effects with random intercepts model ####
# Fit a linear mixed effects model (random intercepts only) for the fixed effect of 
# growth temperature on relative date of flowering and random effect of genotype intercepts
model1.3 <- lmer(relativedate ~ poly(ctemperature, 2, raw = T) + (1|loc) + (1|genotype), 
                 REML = FALSE, data = flowerdata)

# Check model summary and R squared values
summary(model1.3)
r.squaredGLMM(model1.3)

# Predict values based on the model fit using the predict function
temperature_pred$fit1.3 <- predict(model1.3, newdata = temperature_pred, re.form = NA)

# Make a prediction for the population-level mean reaction norm 
# and append it to the flowerdata dataset
flowerdata$pred_pop1.3  <- predict(model1.3, re.form = NA)
# Make predictions for each genotype-level reaction norm
flowerdata$pred_geno1.3 <- predict(model1.3, re.form = ~(1|genotype))

# Plot predicted genotype reaction norms over the raw data, along with the overall mean
ggplot(temperature_pred, aes(x = ctemperature, y = fit1.3)) +
  geom_line(data = flowerdata, aes(y = pred_geno1.3, group = genotype, colour = genotype), lty = 2) +
  geom_line(data = flowerdata,
            aes(y = relativedate, group = genotype, colour = genotype)) +
  geom_line(size = 2) +
  ylab("Relative date of flowering") + xlab("Mean-centred growth temperature") +
  theme_classic()

# Does adding genotype as a random intercept improve model fit?
# Likelihood ratio test
chi2 <- 2*(summary(model1.3)$logLik - summary(model1.2)$logLik)
1-pchisq(chi2, 1)

# AIC comparison
AIC(model1.1, model1.2, model1.3)


#### Quadratic fixed effects with linear random regression model ####
# Fit a linear mixed effects model for the fixed effect of growth temperature on 
# relative date of flowering and random effect of genotype intercepts and slopes
model1.4 <- lmer(relativedate ~ poly(ctemperature, 2, raw = T) + (1|loc) + (1+ctemperature|genotype), 
                 REML = FALSE, data = flowerdata)

# Check model summary and R squared values
summary(model1.4)
r.squaredGLMM(model1.4)

# Predict values based on the model fit using the predict function
temperature_pred$fit1.4 <- predict(model1.4, newdata = temperature_pred, re.form = NA)

# Make a prediction for the population-level mean reaction norm and append it to the flowerdata dataset
flowerdata$pred_pop1.4  <- predict(model1.4, re.form = NA)
# Make predictions for the genotype-level reaction norms
flowerdata$pred_geno1.4 <- predict(model1.4, re.form = ~(1+ctemperature|genotype))

# Plot predicted genotype reaction norms over the raw data, along with the overall mean
ggplot(temperature_pred, aes(x = ctemperature, y = fit1.4)) +
  geom_line(data = flowerdata, aes(y = pred_geno1.4, group = genotype, colour = genotype), lty = 2) +
  geom_line(data = flowerdata, aes(y = relativedate, group = genotype, colour = genotype)) +
  geom_line(size = 2) +
  ylab("Relative date of flowering") + xlab("Mean-centred growth temperature") +
  theme_classic()

# Does adding genotype as a random intercept and slope further improve model fit? 
# Likelihood ratio test
chi2 <- 2*(summary(model1.4)$logLik - summary(model1.3)$logLik)
# The df difference between models can be checked by looking at the df within the models being compared
summary(model1.3)$logLik
summary(model1.4)$logLik
# Note that between model1.3 and model1.4 there is a change of 2 df, so the 
# pchisq change needs to be specified with 2 df rather than 1 as in previous comparisons.
1-pchisq(chi2, 2)

# AIC comparison
AIC(model1.1, model1.2, model1.3, model1.4)


#### Quadratic fixed effects with quadratic random regression model ####
# Fit a linear mixed effects model for the fixed effect of growth temperature on 
# relative date of flowering and random effect of genotype intercepts, slopes, and curvature
model1.5 <- lmer(relativedate ~ poly(ctemperature, 2, raw = T) + (1|loc) +
                   (1 + ctemperature + I(ctemperature^2)|genotype), 
                 REML = FALSE, data = flowerdata)

# Check model summary and R squared values
summary(model1.5)
r.squaredGLMM(model1.5)

# Predict values based on the model fit using the predict function
temperature_pred$fit1.5 <- predict(model1.5, newdata = temperature_pred, re.form = NA)

# Make a prediction for the population-level mean reaction norm and append it to the flowerdata dataset
flowerdata$pred_pop1.5  <- predict(model1.5, re.form = NA)

# Unfortunately, to coerce the predict function to work for a complex random effect, 
# the model needs to be specified without the second random effect (1|loc)
model1.5a <- lmer(relativedate ~ poly(ctemperature, 2, raw = T) +
                    (1 + ctemperature + I(ctemperature^2)|genotype), 
                  REML = FALSE, data = flowerdata)

# We can check whether omitting the (1|loc) random effect changes the fixed effect
# coefficients greatly before interpreting the plot without it
summary(model1.5)$coef
summary(model1.5a)$coef

# Make predictions for the genotype-level reaction norms
flowerdata$pred_geno1.5 <- predict(model1.5a, re.form = NULL)

ggplot(temperature_pred, aes(x = ctemperature, y = fit1.5)) +
  geom_line(data = flowerdata, aes(y = pred_geno1.5, group = genotype, colour = genotype), lty = 2) +
  geom_line(data = flowerdata, aes(y = relativedate, group = genotype, colour = genotype)) +
  geom_line(size = 2) +
  ylab("Relative date of flowering") + xlab("Mean-centred growth temperature") +
  theme_classic()

# Does adding genotype as a random intercept, slope, and curvature further improve model fit? 
# Likelihood ratio test
chi2 <- 2*(summary(model1.5)$logLik - summary(model1.4)$logLik)
# The df difference between models can be checked by looking at the df within the models being compared
summary(model1.4)$logLik
summary(model1.5)$logLik
# Note that between model1.3 and model1.4 there is a change of 3 df, so the 
# pchisq change needs to be specified with 3 df rather than 1 or 2 as in previous comparisons.
1-pchisq(chi2, 3)

# AIC comparison
AIC(model1.1, model1.2, model1.3, model1.4, model1.5)


#### Best Linear Unbiased Predictors (BLUPs) to rank plasticity ####
# BLUPs represent the response of a given genotype to the fixed effect of temperature 
# as the difference between that genotype’s predicted response and the population-level 
# average predicted response. Here, we calculate and plot BLUPs for ranking plasticity.
genotype_blups <- ranef(model1.4)$`genotype`
genotype_index <- as.factor(c(1:20))
genotype_data  <- cbind(genotype_index, genotype_blups)
colnames(genotype_data) <- c("genotype", "BLUP_int", "BLUP_slope")

# BLUPs by intercept
ggplot(genotype_data, aes(genotype, BLUP_int)) + 
  geom_point(aes(group = genotype, colour = genotype), size = 4) + 
  ylab("BLUP intercept estimate") +
  geom_hline(yintercept = 0, lty = 2) + theme_classic()

# BLUPs by slope
ggplot(genotype_data, aes(genotype, BLUP_slope)) + 
  geom_point(aes(group = genotype, colour = genotype), size = 4) + 
  ylab("Plasticity (BLUP slope estimate)") +
  geom_hline(yintercept = 0, lty = 2) + theme_classic()

# Add the BLUP slopes for the genotypes to the population average
pop_av_slope <- fixef(model1.4)[2]
genotype_data$genotype_slopes <- genotype_blups$ctemperature + pop_av_slope

# BLUPs by slope + population-level average
ggplot(genotype_data, aes(genotype, genotype_slopes)) + 
  geom_point(aes(group = genotype, colour = genotype), size = 4) + 
  ylab("Plasticity (population-average + BLUP slope estimate)") +
  geom_hline(yintercept = 0, lty = 2) + theme_classic()

# Correlation between BLUP intercepts and slopes
ggplot(genotype_data, aes(BLUP_int, BLUP_slope)) +
  geom_point(aes(group = genotype, colour = genotype), size = 4) +
  xlab("BLUP intercept estimate") +
  ylab("BLUP slope estimate") +
  theme_classic()

# Rank the BLUPs in order
# Sort BLUPs by slope of most to least plastic
genotype_data$genotype_ordered <- factor(genotype_data$genotype, 
                                         levels = genotype_data$genotype[order(genotype_data$BLUP_slope)])
ggplot(genotype_data, aes(genotype_ordered, BLUP_slope)) +
  geom_bar(stat = "identity", aes(group = genotype, fill = genotype)) +
  xlab("Genotype (in ranked order of plasticity)") +
  ylab("Plasticity (BLUP slope estimate)") +
  theme_classic()

# Another way to visualise the plasticity rank for negative data is by adding
# the BLUP slope values to the population-level average effect of temperature
ggplot(genotype_data, aes(genotype_ordered, genotype_slopes)) +
  geom_bar(stat = "identity", aes(group = genotype, fill = genotype)) +
  xlab("Genotype (in ranked order of plasticity)") +
  ylab("Plasticity (population-average + BLUP slope estimate)") +
  theme_classic()

```



# Application III: data visualization, add fitted equation to plots

How are random effects being included in the linear mixed model along with the concept behind their calculation: https://stats.stackexchange.com/questions/567588/how-are-random-effects-being-included-in-the-linear-mixed-model-along-with-the-c

make_sense_of_random_effects: http://www.alexanderdemos.org/Class14.html#Practice_data_to_make_sense_of_random_effects

```{R}
library(lme4);
data(sleepstudy) #in lme4 

## linear regression 
library(ggpmisc) #https://stackoverflow.com/questions/7549694/add-regression-line-equation-and-r2-on-graph
#https://stackoverflow.com/questions/38722202/how-do-i-change-the-number-of-decimal-places-on-axis-labels-in-ggplot2
scaleFUN <- function(x) sprintf("%.1f", x)
ggplot(sleepstudy,aes(x=Days,y=Reaction,group=Subject,col=Subject))+
  #geom_line(col='black')+
  geom_smooth(method=lm,level=0.99,fill = "grey",alpha=0.1)+
  theme_classic(base_size = 12)+
  xlab('Days')+ylab('Reaction')+#scale_x_log10()+
  geom_point()+
  scale_y_continuous(labels=scaleFUN)+
  stat_poly_eq(use_label(c("eq", "adj.R2", "f", "p", "n"))) 
  #stat_poly_eq(use_label(c("eq", "adj.R2","p"))) 
  #stat_poly_eq(use_label(c("adj.R2","p"))) 

## mixed model
library(lme4)
fm1 <- lmer(Reaction~Days+(1|Subject),sleepstudy, REML=FALSE)
fm2 <- lmer(Reaction~Days+(Days|Subject),sleepstudy, REML=FALSE)

sleepstudy$fm1.fitted<-predict(fm1)
sleepstudy$fm2.fitted<-predict(fm2)

FittedlmPlot1 <-ggplot()+
  #facet_grid(.~Subject, labeller=label_both)+
  facet_wrap(.~Subject, labeller=label_both,ncol=3)+
  geom_line(data = sleepstudy, aes(x = Days, y =fm1.fitted))+
  geom_point(data = sleepstudy, aes(x = Days, y =Reaction, group=Subject,colour = Subject), size=3)+
  xlab("Days")+ylab("Reaction")+theme_classic()
FittedlmPlot1

FittedlmPlot2 <-ggplot()+
  #facet_grid(.~Subject, labeller=label_both)+
  facet_wrap(.~Subject, labeller=label_both,ncol=3)+
  geom_line(data = sleepstudy, aes(x = Days, y =fm2.fitted))+
  geom_point(data = sleepstudy, aes(x = Days, y =Reaction, group=Subject,colour = Subject), size=3)+
  xlab("Days")+ylab("Reaction")+theme_classic()
FittedlmPlot2

fit=fm2;
rancoefs<-ranef(fit)
eq.ab<-as.data.frame(t(sapply(1:nrow(rancoefs$Subject),function(i){
    if(ncol(rancoefs$Subject)==1){ c(fixef(fit)[1],rancoefs$Subject[[1]][i])} #intercept only
    else{c(fixef(fit)[1]+rancoefs$Subject[[1]][i],rancoefs$Subject[[2]][i])}
  })))

m=as.numeric(eq.ab[1,])
eq <- substitute(italic(y) == a + b %.% italic(x),
                   list(a = format(m[1], digits = 2),
                        b = format(m[2], digits = 2)))
as.character(as.expression(eq));

all.eqs<-lapply(1:nrow(eq.ab),function(i){
  m=as.numeric(eq.ab[i,])
  eq <- substitute(italic(y) == a + b %.% italic(x),
                   list(a = format(m[1], digits = 2),
                        b = format(m[2], digits = 2))) 
  
  as.character(as.expression(eq))
})       
data_text=data.frame(label=unlist(all.eqs),x=rep(2,length(all.eqs)),y=rep(200,length(all.eqs)),
                     Subject=factor(levels(sleepstudy$Subject),levels=levels(sleepstudy$Subject)))
#FittedlmPlot2 + annotate('text',label=as.character(as.expression(eq)),parse=T,x=200,y=1)
FittedlmPlot2 + geom_text(data=data_text,mapping=aes(x=x,y=y,label=label),parse=T)
```


References:

- hiercourse, https://www.hiercourse.com/#two
- hiercourse, https://www.hiercourse.com/#workshops
- variancePartition, https://bioconductor.org/packages/release/bioc/html/variancePartition.html
- Arnold, Pieter A., Loeske EB Kruuk, and Adrienne B. Nicotra. "How to analyse plant phenotypic plasticity in response to a changing climate." New Phytologist 222.3 (2019): 1235-1241. https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.15656
- Arnqvist, Göran. "Mixed models offer no freedom from degrees of freedom." Trends in ecology & evolution 35.4 (2020): 329-335. https://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347%2819%2930346-5
- Mixed Effects Models 1: Random Intercept by Yury Zablotski. https://yury-zablotski.netlify.app/post/mixed-effects-models-1/
